"""
Unified Search Service - Orchestrates all text extraction and matching capabilities.

This service provides a comprehensive search interface that combines text extraction,
fuzzy matching, regex pattern matching, and confidence scoring into unified workflows
with optimization, caching, and result management capabilities.
"""

import logging
import time
import asyncio
from typing import Dict, List, Optional, Tuple, Any, Union, Set
from enum import Enum
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib
from collections import defaultdict

from .text_extraction_service import TextExtractionService, ExtractionMethod
from .fuzzy_matcher import FuzzyMatcher, MatchingConfiguration, MatchResult
from .regex_pattern_service import RegexPatternService, PatternType, PatternMatch
from .match_scoring_service import MatchScoringService, MatchForScoring, ConfidenceBreakdown
from ..utils.text_processing import measure_processing_time, get_text_fingerprint

logger = logging.getLogger(__name__)


class SearchStrategy(Enum):
    """Available search strategies."""
    FUZZY_ONLY = "fuzzy_only"
    PATTERN_ONLY = "pattern_only"
    COMBINED = "combined"
    HIERARCHICAL = "hierarchical"
    ADAPTIVE = "adaptive"


class SearchMode(Enum):
    """Search processing modes."""
    SEQUENTIAL = "sequential"
    PARALLEL = "parallel"
    BATCH = "batch"
    STREAMING = "streaming"


class ResultRanking(Enum):
    """Result ranking methods."""
    CONFIDENCE = "confidence"
    RELEVANCE = "relevance"
    PAGE_ORDER = "page_order"
    MATCH_TYPE = "match_type"
    HYBRID = "hybrid"


@dataclass
class SearchConfiguration:
    """Comprehensive search configuration."""
    # Strategy settings
    strategy: SearchStrategy = SearchStrategy.COMBINED
    mode: SearchMode = SearchMode.PARALLEL
    ranking: ResultRanking = ResultRanking.HYBRID
    
    # Text extraction settings
    extraction_method: ExtractionMethod = ExtractionMethod.AUTO
    enable_ocr_fallback: bool = True
    
    # Fuzzy matching settings
    fuzzy_config: Optional[MatchingConfiguration] = None
    enable_fuzzy_matching: bool = True
    
    # Pattern matching settings
    pattern_types: Optional[List[PatternType]] = None
    enable_pattern_validation: bool = True
    custom_patterns: Optional[List[str]] = None
    
    # Confidence scoring settings
    confidence_threshold: float = 0.7
    enable_confidence_calibration: bool = True
    
    # Performance settings
    max_workers: int = 4
    enable_caching: bool = True
    batch_size: int = 100
    timeout_seconds: int = 300
    
    # Result processing settings
    max_results_per_page: int = 1000
    enable_deduplication: bool = True
    enable_clustering: bool = True
    context_window: int = 150
    
    # Advanced settings
    enable_cross_page_analysis: bool = False
    enable_statistical_analysis: bool = True
    enable_progressive_results: bool = False


@dataclass
class UnifiedSearchResult:
    """Unified search result combining all match types."""
    search_term: str
    matched_text: str
    confidence_score: float
    page_number: int
    match_type: str  # 'fuzzy', 'pattern', 'exact'
    source_service: str  # 'fuzzy_matcher', 'regex_pattern_service'
    position_info: Dict[str, int]
    context: str
    
    # Detailed scoring information
    confidence_breakdown: Optional[ConfidenceBreakdown] = None
    
    # Source-specific metadata
    fuzzy_metadata: Optional[Dict[str, Any]] = None
    pattern_metadata: Optional[Dict[str, Any]] = None
    
    # Processing metadata
    processing_time: float = 0.0
    extraction_source: str = ""
    validation_passed: Optional[bool] = None
    
    # Clustering and deduplication
    cluster_id: Optional[str] = None
    is_duplicate: bool = False
    duplicate_of: Optional[str] = None


@dataclass
class SearchSession:
    """Search session tracking and management."""
    session_id: str
    search_terms: List[str]
    configuration: SearchConfiguration
    start_time: float
    total_pages: int = 0
    processed_pages: int = 0
    total_matches: int = 0
    processing_stats: Dict[str, Any] = field(default_factory=dict)
    error_log: List[str] = field(default_factory=list)
    cache_hits: int = 0


class UnifiedSearchService:
    """Unified search service orchestrating all text extraction and matching capabilities.
    
    This service provides a comprehensive search interface that intelligently combines:
    - Text extraction (text layer + OCR)
    - Fuzzy text matching with advanced algorithms
    - Regex pattern matching for sensitive data
    - Multi-factor confidence scoring
    - Result optimization and management
    """
    
    def __init__(self,
                 text_extraction_service: Optional[TextExtractionService] = None,
                 fuzzy_matcher: Optional[FuzzyMatcher] = None,
                 regex_pattern_service: Optional[RegexPatternService] = None,
                 match_scoring_service: Optional[MatchScoringService] = None):
        """Initialize the unified search service.
        
        Args:\n            text_extraction_service: Text extraction service instance\n            fuzzy_matcher: Fuzzy matcher instance\n            regex_pattern_service: Regex pattern service instance\n            match_scoring_service: Match scoring service instance\n        \"\"\"\n        # Initialize or use provided services\n        self.text_extraction_service = text_extraction_service or TextExtractionService()\n        self.fuzzy_matcher = fuzzy_matcher or FuzzyMatcher()\n        self.regex_pattern_service = regex_pattern_service or RegexPatternService()\n        self.match_scoring_service = match_scoring_service or MatchScoringService()\n        \n        # Search session management\n        self.active_sessions: Dict[str, SearchSession] = {}\n        \n        # Result caching\n        self.search_cache: Dict[str, Dict[str, Any]] = {}\n        self.cache_stats = {'hits': 0, 'misses': 0, 'size': 0}\n        \n        # Search optimization data\n        self.search_statistics = {\n            'total_searches': 0,\n            'strategy_usage': defaultdict(int),\n            'performance_metrics': defaultdict(list),\n            'error_counts': defaultdict(int)\n        }\n        \n        # Precompiled search profiles for common use cases\n        self.search_profiles = self._initialize_search_profiles()\n        \n        logger.info(\"UnifiedSearchService initialized with all service integrations\")\n    \n    def _initialize_search_profiles(self) -> Dict[str, SearchConfiguration]:\n        \"\"\"Initialize predefined search profiles for common use cases.\n        \n        Returns:\n            Dictionary of search profiles\n        \"\"\"\n        profiles = {}\n        \n        # Fast search profile for large documents\n        profiles['fast'] = SearchConfiguration(\n            strategy=SearchStrategy.FUZZY_ONLY,\n            mode=SearchMode.PARALLEL,\n            extraction_method=ExtractionMethod.TEXT_LAYER,\n            enable_ocr_fallback=False,\n            confidence_threshold=0.8,\n            max_workers=8,\n            enable_clustering=False,\n            enable_cross_page_analysis=False\n        )\n        \n        # Comprehensive search profile for thorough analysis\n        profiles['comprehensive'] = SearchConfiguration(\n            strategy=SearchStrategy.COMBINED,\n            mode=SearchMode.PARALLEL,\n            extraction_method=ExtractionMethod.HYBRID,\n            enable_ocr_fallback=True,\n            confidence_threshold=0.6,\n            enable_pattern_validation=True,\n            enable_confidence_calibration=True,\n            enable_cross_page_analysis=True,\n            enable_statistical_analysis=True\n        )\n        \n        # Sensitive data profile for PII detection\n        profiles['sensitive_data'] = SearchConfiguration(\n            strategy=SearchStrategy.PATTERN_ONLY,\n            pattern_types=[PatternType.SSN, PatternType.CREDIT_CARD, PatternType.PHONE, \n                          PatternType.EMAIL, PatternType.BANK_ACCOUNT],\n            enable_pattern_validation=True,\n            confidence_threshold=0.9,\n            enable_confidence_calibration=True,\n            context_window=200\n        )\n        \n        # Balanced profile for general use\n        profiles['balanced'] = SearchConfiguration(\n            strategy=SearchStrategy.HIERARCHICAL,\n            mode=SearchMode.PARALLEL,\n            confidence_threshold=0.75,\n            enable_deduplication=True,\n            enable_clustering=True,\n            max_workers=4\n        )\n        \n        return profiles\n    \n    @measure_processing_time\n    def search_document(\n        self,\n        pdf_path: str,\n        search_terms: List[str],\n        config: Optional[SearchConfiguration] = None,\n        profile_name: Optional[str] = None,\n        session_id: Optional[str] = None,\n        progress_callback: Optional[callable] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Perform unified search across a document.\n        \n        Args:\n            pdf_path: Path to the PDF document\n            search_terms: List of terms/patterns to search for\n            config: Search configuration (overrides profile)\n            profile_name: Name of predefined search profile to use\n            session_id: Optional session identifier for tracking\n            progress_callback: Optional callback for progress updates\n            \n        Returns:\n            Comprehensive search results dictionary\n        \"\"\"\n        start_time = time.time()\n        \n        try:\n            # Resolve configuration\n            search_config = self._resolve_search_configuration(config, profile_name)\n            \n            # Create or update session\n            session_id = session_id or self._generate_session_id(pdf_path, search_terms)\n            search_session = self._create_search_session(session_id, search_terms, search_config)\n            \n            logger.info(f\"Starting unified search for {len(search_terms)} terms using strategy: {search_config.strategy.value}\")\n            \n            # Check cache if enabled\n            cache_key = None\n            if search_config.enable_caching:\n                cache_key = self._generate_cache_key(pdf_path, search_terms, search_config)\n                cached_result = self._get_cached_result(cache_key)\n                if cached_result:\n                    self.cache_stats['hits'] += 1\n                    search_session.cache_hits += 1\n                    cached_result['from_cache'] = True\n                    return cached_result\n                self.cache_stats['misses'] += 1\n            \n            # Step 1: Extract text from document\n            extraction_result = self._extract_document_text(\n                pdf_path, search_config, search_session, progress_callback\n            )\n            \n            if not extraction_result['success']:\n                return self._create_error_result(\n                    f\"Text extraction failed: {extraction_result['error']}\",\n                    search_session\n                )\n            \n            text_pages = extraction_result['pages']\n            search_session.total_pages = len(text_pages)\n            \n            # Step 2: Execute search strategy\n            search_results = self._execute_search_strategy(\n                text_pages, search_terms, search_config, search_session, progress_callback\n            )\n            \n            # Step 3: Apply confidence scoring\n            if search_config.enable_confidence_calibration:\n                search_results = self._apply_confidence_scoring(\n                    search_results, search_config, extraction_result.get('ocr_confidence_data')\n                )\n            \n            # Step 4: Post-process results\n            final_results = self._post_process_results(\n                search_results, search_config, search_session\n            )\n            \n            # Step 5: Generate comprehensive response\n            response = self._generate_search_response(\n                final_results, search_session, extraction_result, start_time\n            )\n            \n            # Cache result if enabled\n            if search_config.enable_caching and cache_key:\n                self._cache_result(cache_key, response)\n            \n            # Update statistics\n            self._update_search_statistics(search_config.strategy, time.time() - start_time, len(final_results))\n            \n            return response\n            \n        except Exception as e:\n            logger.error(f\"Unified search failed: {e}\")\n            if session_id in self.active_sessions:\n                self.active_sessions[session_id].error_log.append(str(e))\n            return self._create_error_result(str(e), self.active_sessions.get(session_id))\n    \n    def _resolve_search_configuration(\n        self, \n        config: Optional[SearchConfiguration], \n        profile_name: Optional[str]\n    ) -> SearchConfiguration:\n        \"\"\"Resolve search configuration from config object or profile name.\n        \n        Args:\n            config: Direct configuration object\n            profile_name: Name of predefined profile\n            \n        Returns:\n            Resolved search configuration\n        \"\"\"\n        if config:\n            return config\n        elif profile_name and profile_name in self.search_profiles:\n            return self.search_profiles[profile_name]\n        else:\n            return self.search_profiles['balanced']  # Default profile\n    \n    def _create_search_session(self, session_id: str, search_terms: List[str], config: SearchConfiguration) -> SearchSession:\n        \"\"\"Create and register a search session.\n        \n        Args:\n            session_id: Session identifier\n            search_terms: Search terms\n            config: Search configuration\n            \n        Returns:\n            Created search session\n        \"\"\"\n        session = SearchSession(\n            session_id=session_id,\n            search_terms=search_terms,\n            configuration=config,\n            start_time=time.time()\n        )\n        \n        self.active_sessions[session_id] = session\n        return session\n    \n    def _extract_document_text(\n        self, \n        pdf_path: str, \n        config: SearchConfiguration, \n        session: SearchSession,\n        progress_callback: Optional[callable]\n    ) -> Dict[str, Any]:\n        \"\"\"Extract text from document using configured extraction method.\n        \n        Args:\n            pdf_path: Path to PDF document\n            config: Search configuration\n            session: Search session\n            progress_callback: Progress callback\n            \n        Returns:\n            Text extraction result\n        \"\"\"\n        try:\n            # Configure text extraction\n            extraction_pages = self.text_extraction_service.extract_text(\n                pdf_path=pdf_path,\n                method=config.extraction_method,\n                pages=None,  # Extract all pages\n                enable_caching=config.enable_caching,\n                progress_callback=progress_callback\n            )\n            \n            if not extraction_pages:\n                return {'success': False, 'error': 'No pages extracted from document'}\n            \n            # Convert extraction results to search format\n            text_pages = []\n            ocr_confidence_data = {}\n            \n            for page_result in extraction_pages:\n                if page_result.results:\n                    primary_result = page_result.results[0]\n                    text_pages.append({\n                        'page_number': page_result.page_number,\n                        'text': page_result.combined_text,\n                        'extraction_source': page_result.recommended_source.value,\n                        'confidence': page_result.combined_confidence,\n                        'processing_time': page_result.processing_time\n                    })\n                    \n                    # Store OCR confidence if available\n                    if hasattr(primary_result, 'ocr_confidence') and primary_result.ocr_confidence:\n                        ocr_confidence_data[page_result.page_number] = primary_result.ocr_confidence\n            \n            session.processing_stats['text_extraction'] = {\n                'pages_extracted': len(text_pages),\n                'extraction_method': config.extraction_method.value,\n                'total_processing_time': sum(page.get('processing_time', 0) for page in text_pages)\n            }\n            \n            return {\n                'success': True,\n                'pages': text_pages,\n                'ocr_confidence_data': ocr_confidence_data,\n                'extraction_statistics': self.text_extraction_service.get_extraction_statistics(extraction_pages)\n            }\n            \n        except Exception as e:\n            logger.error(f\"Text extraction failed: {e}\")\n            session.error_log.append(f\"Text extraction error: {str(e)}\")\n            return {'success': False, 'error': str(e)}\n    \n    def _execute_search_strategy(\n        self,\n        text_pages: List[Dict[str, Any]],\n        search_terms: List[str],\n        config: SearchConfiguration,\n        session: SearchSession,\n        progress_callback: Optional[callable]\n    ) -> List[UnifiedSearchResult]:\n        \"\"\"Execute the configured search strategy.\n        \n        Args:\n            text_pages: Extracted text pages\n            search_terms: Terms to search for\n            config: Search configuration\n            session: Search session\n            progress_callback: Progress callback\n            \n        Returns:\n            List of unified search results\n        \"\"\"\n        all_results = []\n        \n        try:\n            if config.strategy == SearchStrategy.FUZZY_ONLY:\n                all_results = self._execute_fuzzy_search(text_pages, search_terms, config)\n            \n            elif config.strategy == SearchStrategy.PATTERN_ONLY:\n                all_results = self._execute_pattern_search(text_pages, search_terms, config)\n            \n            elif config.strategy == SearchStrategy.COMBINED:\n                fuzzy_results = self._execute_fuzzy_search(text_pages, search_terms, config)\n                pattern_results = self._execute_pattern_search(text_pages, search_terms, config)\n                all_results = fuzzy_results + pattern_results\n            \n            elif config.strategy == SearchStrategy.HIERARCHICAL:\n                all_results = self._execute_hierarchical_search(text_pages, search_terms, config)\n            \n            elif config.strategy == SearchStrategy.ADAPTIVE:\n                all_results = self._execute_adaptive_search(text_pages, search_terms, config)\n            \n            session.total_matches = len(all_results)\n            session.processing_stats['search_execution'] = {\n                'strategy_used': config.strategy.value,\n                'total_matches_found': len(all_results),\n                'search_terms_processed': len(search_terms)\n            }\n            \n            if progress_callback:\n                progress_callback(len(text_pages), len(text_pages))  # Mark search as complete\n            \n            return all_results\n            \n        except Exception as e:\n            logger.error(f\"Search strategy execution failed: {e}\")\n            session.error_log.append(f\"Search execution error: {str(e)}\")\n            return []\n    \n    def _execute_fuzzy_search(self, text_pages: List[Dict[str, Any]], search_terms: List[str], config: SearchConfiguration) -> List[UnifiedSearchResult]:\n        \"\"\"Execute fuzzy search using the fuzzy matcher.\n        \n        Args:\n            text_pages: Text pages to search\n            search_terms: Terms to search for\n            config: Search configuration\n            \n        Returns:\n            List of fuzzy search results\n        \"\"\"\n        try:\n            # Use fuzzy matcher configuration if provided\n            fuzzy_config = config.fuzzy_config or MatchingConfiguration(\n                threshold=int(config.confidence_threshold * 100),\n                strategy=MatchingConfiguration().strategy,\n                enable_parallel_processing=config.mode == SearchMode.PARALLEL\n            )\n            \n            # Get OCR confidence data for integration\n            ocr_confidence_data = {}\n            for page in text_pages:\n                if 'confidence' in page:\n                    ocr_confidence_data[page['page_number']] = page['confidence']\n            \n            # Execute fuzzy matching\n            fuzzy_matches = self.fuzzy_matcher.find_matches(\n                search_terms=search_terms,\n                text_pages=text_pages,\n                custom_config=fuzzy_config,\n                ocr_confidence_data=ocr_confidence_data\n            )\n            \n            # Convert to unified results\n            unified_results = []\n            for match in fuzzy_matches:\n                unified_result = UnifiedSearchResult(\n                    search_term=match.search_term,\n                    matched_text=match.matched_text,\n                    confidence_score=match.confidence_score,\n                    page_number=match.page_number,\n                    match_type='fuzzy',\n                    source_service='fuzzy_matcher',\n                    position_info=match.position_info,\n                    context=match.match_context,\n                    fuzzy_metadata={\n                        'algorithm_used': match.algorithm_used,\n                        'similarity_scores': match.similarity_scores,\n                        'preprocessing_applied': match.preprocessing_applied,\n                        'needs_approval': match.needs_approval\n                    },\n                    processing_time=match.processing_time or 0.0,\n                    extraction_source=getattr(match, 'extraction_source', ''),\n                    cluster_id=match.cluster_id\n                )\n                unified_results.append(unified_result)\n            \n            return unified_results\n            \n        except Exception as e:\n            logger.error(f\"Fuzzy search execution failed: {e}\")\n            return []\n    \n    def _execute_pattern_search(self, text_pages: List[Dict[str, Any]], search_terms: List[str], config: SearchConfiguration) -> List[UnifiedSearchResult]:\n        \"\"\"Execute pattern search using the regex pattern service.\n        \n        Args:\n            text_pages: Text pages to search\n            search_terms: Terms to search for (used as custom patterns if needed)\n            config: Search configuration\n            \n        Returns:\n            List of pattern search results\n        \"\"\"\n        try:\n            # Execute pattern matching\n            pattern_matches = self.regex_pattern_service.find_pattern_matches(\n                text_pages=text_pages,\n                pattern_types=config.pattern_types,\n                enable_validation=config.enable_pattern_validation,\n                confidence_threshold=config.confidence_threshold\n            )\n            \n            # Convert to unified results\n            unified_results = []\n            for match in pattern_matches:\n                unified_result = UnifiedSearchResult(\n                    search_term=match.pattern_name,  # Use pattern name as search term\n                    matched_text=match.matched_text,\n                    confidence_score=match.confidence_score,\n                    page_number=match.page_number,\n                    match_type='pattern',\n                    source_service='regex_pattern_service',\n                    position_info=match.position_info,\n                    context=match.context,\n                    pattern_metadata={\n                        'pattern_name': match.pattern_name,\n                        'pattern_type': match.pattern_type.value,\n                        'validation_passed': match.validation_passed,\n                        'validation_details': match.validation_details\n                    },\n                    processing_time=match.processing_time,\n                    validation_passed=match.validation_passed\n                )\n                unified_results.append(unified_result)\n            \n            return unified_results\n            \n        except Exception as e:\n            logger.error(f\"Pattern search execution failed: {e}\")\n            return []\n    \n    def _execute_hierarchical_search(self, text_pages: List[Dict[str, Any]], search_terms: List[str], config: SearchConfiguration) -> List[UnifiedSearchResult]:\n        \"\"\"Execute hierarchical search (exact → fuzzy → pattern).\n        \n        Args:\n            text_pages: Text pages to search\n            search_terms: Terms to search for\n            config: Search configuration\n            \n        Returns:\n            List of hierarchical search results\n        \"\"\"\n        try:\n            all_results = []\n            \n            # Step 1: Try exact matches first (using fuzzy matcher with 100% threshold)\n            exact_config = config.fuzzy_config or MatchingConfiguration()\n            exact_config.threshold = 100  # Exact matches only\n            \n            exact_matches = self.fuzzy_matcher.find_matches(\n                search_terms=search_terms,\n                text_pages=text_pages,\n                custom_config=exact_config\n            )\n            \n            # Convert exact matches\n            for match in exact_matches:\n                if match.confidence_score >= 0.99:  # Truly exact matches\n                    unified_result = UnifiedSearchResult(\n                        search_term=match.search_term,\n                        matched_text=match.matched_text,\n                        confidence_score=1.0,  # Boost exact matches\n                        page_number=match.page_number,\n                        match_type='exact',\n                        source_service='fuzzy_matcher',\n                        position_info=match.position_info,\n                        context=match.match_context,\n                        fuzzy_metadata={'is_exact_match': True},\n                        processing_time=match.processing_time or 0.0\n                    )\n                    all_results.append(unified_result)\n            \n            # Step 2: If no exact matches, try fuzzy matching\n            if not all_results or len(all_results) < len(search_terms):\n                fuzzy_results = self._execute_fuzzy_search(text_pages, search_terms, config)\n                # Filter out exact matches we already found\n                exact_match_texts = {(r.search_term, r.matched_text, r.page_number) for r in all_results}\n                fuzzy_results = [\n                    r for r in fuzzy_results \n                    if (r.search_term, r.matched_text, r.page_number) not in exact_match_texts\n                ]\n                all_results.extend(fuzzy_results)\n            \n            # Step 3: Always run pattern matching for additional sensitive data detection\n            pattern_results = self._execute_pattern_search(text_pages, search_terms, config)\n            all_results.extend(pattern_results)\n            \n            return all_results\n            \n        except Exception as e:\n            logger.error(f\"Hierarchical search execution failed: {e}\")\n            return []\n    \n    def _execute_adaptive_search(self, text_pages: List[Dict[str, Any]], search_terms: List[str], config: SearchConfiguration) -> List[UnifiedSearchResult]:\n        \"\"\"Execute adaptive search that adjusts strategy based on document characteristics.\n        \n        Args:\n            text_pages: Text pages to search\n            search_terms: Terms to search for\n            config: Search configuration\n            \n        Returns:\n            List of adaptive search results\n        \"\"\"\n        try:\n            # Analyze document characteristics\n            doc_analysis = self._analyze_document_characteristics(text_pages)\n            \n            # Adapt strategy based on analysis\n            if doc_analysis['ocr_heavy'] and doc_analysis['low_confidence']:\n                # Use more permissive fuzzy matching for poor quality documents\n                adaptive_config = config\n                adaptive_config.confidence_threshold = max(0.5, config.confidence_threshold - 0.2)\n                return self._execute_fuzzy_search(text_pages, search_terms, adaptive_config)\n            \n            elif doc_analysis['structured_content']:\n                # Use pattern matching for structured documents\n                return self._execute_pattern_search(text_pages, search_terms, config)\n            \n            elif doc_analysis['high_quality']:\n                # Use hierarchical approach for high-quality documents\n                return self._execute_hierarchical_search(text_pages, search_terms, config)\n            \n            else:\n                # Fall back to combined approach\n                return self._execute_combined_search(text_pages, search_terms, config)\n                \n        except Exception as e:\n            logger.error(f\"Adaptive search execution failed: {e}\")\n            # Fall back to combined search\n            return self._execute_combined_search(text_pages, search_terms, config)\n    \n    def _execute_combined_search(self, text_pages: List[Dict[str, Any]], search_terms: List[str], config: SearchConfiguration) -> List[UnifiedSearchResult]:\n        \"\"\"Execute combined search (both fuzzy and pattern).\n        \n        Args:\n            text_pages: Text pages to search\n            search_terms: Terms to search for\n            config: Search configuration\n            \n        Returns:\n            List of combined search results\n        \"\"\"\n        try:\n            fuzzy_results = self._execute_fuzzy_search(text_pages, search_terms, config)\n            pattern_results = self._execute_pattern_search(text_pages, search_terms, config)\n            return fuzzy_results + pattern_results\n            \n        except Exception as e:\n            logger.error(f\"Combined search execution failed: {e}\")\n            return []\n    \n    def _analyze_document_characteristics(self, text_pages: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"Analyze document characteristics to inform adaptive search strategy.\n        \n        Args:\n            text_pages: Text pages to analyze\n            \n        Returns:\n            Dictionary with document characteristics\n        \"\"\"\n        try:\n            analysis = {\n                'total_pages': len(text_pages),\n                'total_text_length': sum(len(page.get('text', '')) for page in text_pages),\n                'average_confidence': 0.0,\n                'ocr_pages': 0,\n                'text_layer_pages': 0,\n                'low_confidence': False,\n                'high_quality': False,\n                'ocr_heavy': False,\n                'structured_content': False\n            }\n            \n            confidences = []\n            structured_indicators = 0\n            \n            for page in text_pages:\n                confidence = page.get('confidence', 1.0)\n                confidences.append(confidence)\n                \n                extraction_source = page.get('extraction_source', '')\n                if 'ocr' in extraction_source.lower():\n                    analysis['ocr_pages'] += 1\n                else:\n                    analysis['text_layer_pages'] += 1\n                \n                # Check for structured content indicators\n                text = page.get('text', '')\n                if any(indicator in text for indicator in ['|', ':', '=', '->', '#', '\\t']):\n                    structured_indicators += 1\n            \n            if confidences:\n                analysis['average_confidence'] = sum(confidences) / len(confidences)\n                analysis['low_confidence'] = analysis['average_confidence'] < 0.7\n                analysis['high_quality'] = analysis['average_confidence'] >= 0.9\n            \n            analysis['ocr_heavy'] = analysis['ocr_pages'] > analysis['text_layer_pages']\n            analysis['structured_content'] = structured_indicators > len(text_pages) * 0.3\n            \n            return analysis\n            \n        except Exception as e:\n            logger.debug(f\"Error analyzing document characteristics: {e}\")\n            return {'total_pages': len(text_pages), 'average_confidence': 0.7}\n    \n    def _apply_confidence_scoring(self, results: List[UnifiedSearchResult], config: SearchConfiguration, ocr_confidence_data: Optional[Dict[int, float]]) -> List[UnifiedSearchResult]:\n        \"\"\"Apply advanced confidence scoring to search results.\n        \n        Args:\n            results: Search results to score\n            config: Search configuration\n            ocr_confidence_data: OCR confidence data by page\n            \n        Returns:\n            Results with updated confidence scores\n        \"\"\"\n        try:\n            enhanced_results = []\n            \n            for result in results:\n                # Create match data for scoring\n                match_data = MatchForScoring(\n                    search_term=result.search_term,\n                    matched_text=result.matched_text,\n                    page_number=result.page_number,\n                    position_start=result.position_info.get('start', 0),\n                    position_end=result.position_info.get('end', 0),\n                    context=result.context,\n                    match_type=result.match_type,\n                    fuzzy_confidence=result.confidence_score if result.match_type in ['fuzzy', 'exact'] else None,\n                    ocr_confidence=ocr_confidence_data.get(result.page_number) if ocr_confidence_data else None,\n                    pattern_validation=result.validation_passed,\n                    algorithm_scores=result.fuzzy_metadata.get('similarity_scores') if result.fuzzy_metadata else None\n                )\n                \n                # Calculate enhanced confidence\n                confidence_breakdown = self.match_scoring_service.calculate_confidence_score(match_data)\n                \n                # Update result with enhanced confidence\n                result.confidence_score = confidence_breakdown.final_confidence\n                result.confidence_breakdown = confidence_breakdown\n                \n                enhanced_results.append(result)\n            \n            return enhanced_results\n            \n        except Exception as e:\n            logger.error(f\"Confidence scoring failed: {e}\")\n            return results  # Return original results if scoring fails\n    \n    def _post_process_results(self, results: List[UnifiedSearchResult], config: SearchConfiguration, session: SearchSession) -> List[UnifiedSearchResult]:\n        \"\"\"Post-process search results with deduplication, clustering, and ranking.\n        \n        Args:\n            results: Raw search results\n            config: Search configuration\n            session: Search session\n            \n        Returns:\n            Post-processed results\n        \"\"\"\n        try:\n            processed_results = results.copy()\n            \n            # Step 1: Filter by confidence threshold\n            processed_results = [\n                result for result in processed_results \n                if result.confidence_score >= config.confidence_threshold\n            ]\n            \n            # Step 2: Deduplication\n            if config.enable_deduplication:\n                processed_results = self._deduplicate_results(processed_results)\n            \n            # Step 3: Clustering\n            if config.enable_clustering:\n                processed_results = self._cluster_results(processed_results)\n            \n            # Step 4: Ranking\n            processed_results = self._rank_results(processed_results, config.ranking)\n            \n            # Step 5: Limit results per page\n            if config.max_results_per_page > 0:\n                processed_results = self._limit_results_per_page(processed_results, config.max_results_per_page)\n            \n            session.processing_stats['post_processing'] = {\n                'original_count': len(results),\n                'after_confidence_filter': len([r for r in results if r.confidence_score >= config.confidence_threshold]),\n                'final_count': len(processed_results),\n                'deduplication_enabled': config.enable_deduplication,\n                'clustering_enabled': config.enable_clustering\n            }\n            \n            return processed_results\n            \n        except Exception as e:\n            logger.error(f\"Result post-processing failed: {e}\")\n            session.error_log.append(f\"Post-processing error: {str(e)}\")\n            return results  # Return original results if post-processing fails\n    \n    def _deduplicate_results(self, results: List[UnifiedSearchResult]) -> List[UnifiedSearchResult]:\n        \"\"\"Remove duplicate search results.\n        \n        Args:\n            results: Search results to deduplicate\n            \n        Returns:\n            Deduplicated results\n        \"\"\"\n        seen = set()\n        deduplicated = []\n        \n        for result in results:\n            # Create key based on matched text, page, and position\n            key = (\n                result.matched_text.lower().strip(),\n                result.page_number,\n                result.position_info.get('start', 0)\n            )\n            \n            if key not in seen:\n                seen.add(key)\n                deduplicated.append(result)\n            else:\n                # Mark as duplicate and reference original\n                result.is_duplicate = True\n                result.duplicate_of = str(key)\n        \n        return deduplicated\n    \n    def _cluster_results(self, results: List[UnifiedSearchResult]) -> List[UnifiedSearchResult]:\n        \"\"\"Cluster similar search results.\n        \n        Args:\n            results: Search results to cluster\n            \n        Returns:\n            Results with cluster information\n        \"\"\"\n        try:\n            # Simple clustering based on search terms and similarity\n            clusters = defaultdict(list)\n            \n            for result in results:\n                # Create cluster key based on search term and text similarity\n                cluster_key = result.search_term.lower()\n                clusters[cluster_key].append(result)\n            \n            # Assign cluster IDs\n            for cluster_id, cluster_results in clusters.items():\n                cluster_hash = hashlib.md5(cluster_id.encode()).hexdigest()[:8]\n                for result in cluster_results:\n                    result.cluster_id = cluster_hash\n            \n            return results\n            \n        except Exception as e:\n            logger.debug(f\"Result clustering failed: {e}\")\n            return results\n    \n    def _rank_results(self, results: List[UnifiedSearchResult], ranking_method: ResultRanking) -> List[UnifiedSearchResult]:\n        \"\"\"Rank search results according to specified method.\n        \n        Args:\n            results: Results to rank\n            ranking_method: Ranking method to use\n            \n        Returns:\n            Ranked results\n        \"\"\"\n        try:\n            if ranking_method == ResultRanking.CONFIDENCE:\n                results.sort(key=lambda x: -x.confidence_score)\n            \n            elif ranking_method == ResultRanking.PAGE_ORDER:\n                results.sort(key=lambda x: (x.page_number, x.position_info.get('start', 0)))\n            \n            elif ranking_method == ResultRanking.MATCH_TYPE:\n                # Priority: exact > pattern > fuzzy\n                type_priority = {'exact': 0, 'pattern': 1, 'fuzzy': 2}\n                results.sort(key=lambda x: (type_priority.get(x.match_type, 3), -x.confidence_score))\n            \n            elif ranking_method == ResultRanking.HYBRID:\n                # Combine confidence and match type\n                def hybrid_score(result):\n                    confidence = result.confidence_score\n                    type_bonus = {'exact': 0.1, 'pattern': 0.05, 'fuzzy': 0.0}.get(result.match_type, 0.0)\n                    return confidence + type_bonus\n                \n                results.sort(key=hybrid_score, reverse=True)\n            \n            else:  # Default to confidence\n                results.sort(key=lambda x: -x.confidence_score)\n            \n            return results\n            \n        except Exception as e:\n            logger.debug(f\"Result ranking failed: {e}\")\n            return results\n    \n    def _limit_results_per_page(self, results: List[UnifiedSearchResult], max_per_page: int) -> List[UnifiedSearchResult]:\n        \"\"\"Limit number of results per page.\n        \n        Args:\n            results: Results to limit\n            max_per_page: Maximum results per page\n            \n        Returns:\n            Limited results\n        \"\"\"\n        if max_per_page <= 0:\n            return results\n        \n        page_counts = defaultdict(int)\n        limited_results = []\n        \n        for result in results:\n            page_num = result.page_number\n            if page_counts[page_num] < max_per_page:\n                limited_results.append(result)\n                page_counts[page_num] += 1\n        \n        return limited_results\n    \n    def _generate_search_response(self, results: List[UnifiedSearchResult], session: SearchSession, extraction_result: Dict[str, Any], start_time: float) -> Dict[str, Any]:\n        \"\"\"Generate comprehensive search response.\n        \n        Args:\n            results: Final search results\n            session: Search session\n            extraction_result: Text extraction result\n            start_time: Search start time\n            \n        Returns:\n            Comprehensive search response\n        \"\"\"\n        total_time = time.time() - start_time\n        \n        # Calculate result statistics\n        result_stats = self._calculate_result_statistics(results)\n        \n        # Generate confidence analysis\n        confidence_analysis = self._analyze_result_confidence(results)\n        \n        response = {\n            'success': True,\n            'session_id': session.session_id,\n            'search_configuration': {\n                'strategy': session.configuration.strategy.value,\n                'mode': session.configuration.mode.value,\n                'ranking': session.configuration.ranking.value,\n                'confidence_threshold': session.configuration.confidence_threshold\n            },\n            'results': [\n                {\n                    'search_term': result.search_term,\n                    'matched_text': result.matched_text,\n                    'confidence_score': round(result.confidence_score, 3),\n                    'page_number': result.page_number,\n                    'match_type': result.match_type,\n                    'source_service': result.source_service,\n                    'position': result.position_info,\n                    'context': result.context,\n                    'confidence_level': result.confidence_breakdown.confidence_level.value if result.confidence_breakdown else 'unknown',\n                    'extraction_source': result.extraction_source,\n                    'validation_passed': result.validation_passed,\n                    'cluster_id': result.cluster_id,\n                    'is_duplicate': result.is_duplicate,\n                    'metadata': {\n                        'fuzzy_metadata': result.fuzzy_metadata,\n                        'pattern_metadata': result.pattern_metadata,\n                        'processing_time': result.processing_time\n                    }\n                } for result in results\n            ],\n            'statistics': {\n                'total_matches': len(results),\n                'pages_processed': session.total_pages,\n                'search_terms': len(session.search_terms),\n                'processing_time_seconds': round(total_time, 3),\n                'result_statistics': result_stats,\n                'confidence_analysis': confidence_analysis,\n                'extraction_statistics': extraction_result.get('extraction_statistics', {}),\n                'cache_hits': session.cache_hits,\n                'error_count': len(session.error_log)\n            },\n            'processing_metadata': {\n                'text_extraction': session.processing_stats.get('text_extraction', {}),\n                'search_execution': session.processing_stats.get('search_execution', {}),\n                'post_processing': session.processing_stats.get('post_processing', {}),\n                'errors': session.error_log\n            },\n            'from_cache': False\n        }\n        \n        return response\n    \n    def _calculate_result_statistics(self, results: List[UnifiedSearchResult]) -> Dict[str, Any]:\n        \"\"\"Calculate comprehensive statistics for search results.\n        \n        Args:\n            results: Search results to analyze\n            \n        Returns:\n            Result statistics\n        \"\"\"\n        if not results:\n            return {}\n        \n        # Match type distribution\n        match_types = defaultdict(int)\n        source_services = defaultdict(int)\n        page_distribution = defaultdict(int)\n        confidence_scores = []\n        \n        for result in results:\n            match_types[result.match_type] += 1\n            source_services[result.source_service] += 1\n            page_distribution[result.page_number] += 1\n            confidence_scores.append(result.confidence_score)\n        \n        stats = {\n            'match_type_distribution': dict(match_types),\n            'source_service_distribution': dict(source_services),\n            'pages_with_matches': len(page_distribution),\n            'matches_per_page': dict(page_distribution),\n            'confidence_statistics': {\n                'mean': sum(confidence_scores) / len(confidence_scores),\n                'min': min(confidence_scores),\n                'max': max(confidence_scores),\n                'above_threshold': len([s for s in confidence_scores if s >= 0.8])\n            }\n        }\n        \n        return stats\n    \n    def _analyze_result_confidence(self, results: List[UnifiedSearchResult]) -> Dict[str, Any]:\n        \"\"\"Analyze confidence patterns in search results.\n        \n        Args:\n            results: Search results to analyze\n            \n        Returns:\n            Confidence analysis\n        \"\"\"\n        if not results:\n            return {}\n        \n        # Use match scoring service for comprehensive analysis\n        confidence_breakdowns = [r.confidence_breakdown for r in results if r.confidence_breakdown]\n        \n        if confidence_breakdowns:\n            return self.match_scoring_service.analyze_confidence_trends(confidence_breakdowns)\n        else:\n            return {'error': 'No confidence breakdown data available'}\n    \n    def _generate_session_id(self, pdf_path: str, search_terms: List[str]) -> str:\n        \"\"\"Generate unique session ID.\n        \n        Args:\n            pdf_path: PDF file path\n            search_terms: Search terms\n            \n        Returns:\n            Unique session identifier\n        \"\"\"\n        content = f\"{pdf_path}:{':'.join(sorted(search_terms))}:{time.time()}\"\n        return hashlib.md5(content.encode()).hexdigest()\n    \n    def _generate_cache_key(self, pdf_path: str, search_terms: List[str], config: SearchConfiguration) -> str:\n        \"\"\"Generate cache key for search results.\n        \n        Args:\n            pdf_path: PDF file path\n            search_terms: Search terms\n            config: Search configuration\n            \n        Returns:\n            Cache key\n        \"\"\"\n        # Create key from file, terms, and important config parameters\n        config_key = f\"{config.strategy.value}:{config.confidence_threshold}:{config.extraction_method.value}\"\n        content = f\"{pdf_path}:{':'.join(sorted(search_terms))}:{config_key}\"\n        return hashlib.md5(content.encode()).hexdigest()\n    \n    def _get_cached_result(self, cache_key: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get cached search result.\n        \n        Args:\n            cache_key: Cache key\n            \n        Returns:\n            Cached result or None\n        \"\"\"\n        return self.search_cache.get(cache_key)\n    \n    def _cache_result(self, cache_key: str, result: Dict[str, Any]):\n        \"\"\"Cache search result.\n        \n        Args:\n            cache_key: Cache key\n            result: Result to cache\n        \"\"\"\n        # Simple cache size management\n        if len(self.search_cache) > 1000:\n            # Remove oldest entries (simple FIFO)\n            oldest_keys = list(self.search_cache.keys())[:100]\n            for key in oldest_keys:\n                del self.search_cache[key]\n        \n        self.search_cache[cache_key] = result\n        self.cache_stats['size'] = len(self.search_cache)\n    \n    def _create_error_result(self, error_message: str, session: Optional[SearchSession]) -> Dict[str, Any]:\n        \"\"\"Create error response.\n        \n        Args:\n            error_message: Error description\n            session: Search session (if available)\n            \n        Returns:\n            Error response dictionary\n        \"\"\"\n        return {\n            'success': False,\n            'error': error_message,\n            'session_id': session.session_id if session else None,\n            'results': [],\n            'statistics': {\n                'total_matches': 0,\n                'error_count': len(session.error_log) if session else 1\n            },\n            'from_cache': False\n        }\n    \n    def _update_search_statistics(self, strategy: SearchStrategy, processing_time: float, result_count: int):\n        \"\"\"Update global search statistics.\n        \n        Args:\n            strategy: Search strategy used\n            processing_time: Total processing time\n            result_count: Number of results found\n        \"\"\"\n        self.search_statistics['total_searches'] += 1\n        self.search_statistics['strategy_usage'][strategy.value] += 1\n        self.search_statistics['performance_metrics']['processing_times'].append(processing_time)\n        self.search_statistics['performance_metrics']['result_counts'].append(result_count)\n    \n    def get_search_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive search statistics.\n        \n        Returns:\n            Search statistics dictionary\n        \"\"\"\n        stats = self.search_statistics.copy()\n        \n        # Add cache statistics\n        stats['cache_statistics'] = self.cache_stats.copy()\n        \n        # Add service statistics\n        stats['service_statistics'] = {\n            'text_extraction': self.text_extraction_service.get_extraction_statistics([]),\n            'fuzzy_matcher': {},  # Would need to add method to FuzzyMatcher\n            'regex_patterns': self.regex_pattern_service.get_pattern_statistics(),\n            'match_scoring': self.match_scoring_service.get_scoring_statistics()\n        }\n        \n        # Add active sessions info\n        stats['active_sessions'] = len(self.active_sessions)\n        \n        return stats\n    \n    def clear_cache(self):\n        \"\"\"Clear search result cache.\"\"\"\n        self.search_cache.clear()\n        self.cache_stats = {'hits': 0, 'misses': 0, 'size': 0}\n        logger.info(\"Search cache cleared\")\n    \n    def get_search_profiles(self) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Get available search profiles.\n        \n        Returns:\n            Dictionary of search profile information\n        \"\"\"\n        profile_info = {}\n        \n        for name, config in self.search_profiles.items():\n            profile_info[name] = {\n                'strategy': config.strategy.value,\n                'mode': config.mode.value,\n                'ranking': config.ranking.value,\n                'confidence_threshold': config.confidence_threshold,\n                'extraction_method': config.extraction_method.value,\n                'enable_ocr_fallback': config.enable_ocr_fallback,\n                'enable_pattern_validation': config.enable_pattern_validation,\n                'max_workers': config.max_workers\n            }\n        \n        return profile_info\n    \n    def create_custom_profile(self, name: str, config: SearchConfiguration) -> bool:\n        \"\"\"Create a custom search profile.\n        \n        Args:\n            name: Profile name\n            config: Search configuration\n            \n        Returns:\n            True if profile was created successfully\n        \"\"\"\n        try:\n            self.search_profiles[name] = config\n            logger.info(f\"Created custom search profile: {name}\")\n            return True\n        except Exception as e:\n            logger.error(f\"Failed to create custom profile '{name}': {e}\")\n            return False